{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2018-11-21\n",
    "* had noticed when looking at results from April again that overfitting seems to be happening: validation accuracy is best in the first epoch and then fluctuates or drops, and the validation loss only ever increases\n",
    "* added L2 regularization of kernels to encoder-decoder and dilated temporal convolution models\n",
    "  - first I tried just adding every kind of regularization (bias, activation) and this (not surprisingly in retrospect) blew up the loss and prevented the models from even learning\n",
    "  - then I tried different values for the L2 loss, 0.001 seems to be the sweet spot, also tried {0.1, 0.01, and 0.0001} and they all gave lower validation accuracy, at least for encoder-decoder model with 300 time steps in the input\n",
    "* L2 regularization of kernels does seem to help\n",
    "* also finding that a convolution length of 25 is around the sweet spot for encoder-decoder model\n",
    "  - length 10 slightly worse, 50 no noticeable difference, 200 and 400 are noticeably worse -- \"too much\" context or loss of detail?\n",
    "  - don't have a good intuition for what this is doing, is it really just the kernel dimensions? Because I'm not using the \"causal\" model so there's no attempt to predict the \"future\" in this encoder-decoder (i.e. the next pixel from previous pixels)\n",
    "\n",
    "# 2018-11-22\n",
    "* Dilated TCN still seems to be overfitting. acc on training keeps going up but val loss and val acc peak rapidly then val acc declines while val loss fluctuates\n",
    "* seems like effect was more noticeable when the number of time steps in the spectrogram window was 300 but is still there for 88 time steps\n",
    "* also seems again that a conv_len of 200, 400 does worse than 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Occurs to me that what I should be doing is having a couple of good old' conv layers above the encoder-decoder layers and the dilated TCN layers. In the Lea et al. paper they are actually using outputs from a conv-net for each time step, and the CNN-biLSTM model of course uses conv layers**\n",
    "\n",
    "**In fact what would be an even better test would be to save the weights from the CNN layers learned by the CNN-biLSTM model and then use those same layers but replace the biLSTM part with ED-TCN and dilated TCN**\n",
    "\n",
    "Add CNN layers. Actually works after minimal futzing. I think I'm doing this in a legit way?\n",
    "Seems to be more well-behaved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
